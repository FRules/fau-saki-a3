{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import copy\n",
    "import mdptoolbox\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NUMBER_OF_SLOTS` is a hyper parameter which can be changed to a value between 1 and 6. It makes sense to initialize it with 4 for a 2x2 warehouse and 6 for a 3x2 warehouse. We also define constants for the training file and the reward mapping. The 0th field gets the most reward because the distance is the smallest, decreasing to 1. We also define a cost mapping which is basically needed for measuring the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_SLOTS = 6\n",
    "TRAINING_FILE = \"training_3x2.txt\"\n",
    "REWARD_MAPPING = {\n",
    "    0: 6,\n",
    "    1: 4,\n",
    "    2: 4,\n",
    "    3: 2,\n",
    "    4: 2,\n",
    "    5: 1\n",
    "}\n",
    "COST_MAPPING = {\n",
    "    0: 1,\n",
    "    1: 2,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    4: 3,\n",
    "    5: 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to get the probabilities for a certain action, e. g. for storing red, restoring blue, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_percentage_of_actions_by_training_data(training_file):\n",
    "    actions_dict = {\"SR\": 0, \"SB\": 0, \"SW\": 0, \"RR\": 0, \"RB\": 0, \"RW\": 0}\n",
    "    sum = 0\n",
    "    with open(training_file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "        for row in csv_reader:\n",
    "            action = (row[0][0] + row[1][0]).upper()\n",
    "            actions_dict[action] = actions_dict[action] + 1\n",
    "            sum += 1\n",
    "    probabilities = {\n",
    "        \"SR\": actions_dict[\"SR\"] / sum,\n",
    "        \"SB\": actions_dict[\"SB\"] / sum,\n",
    "        \"SW\": actions_dict[\"SW\"] / sum,\n",
    "        \"RR\": actions_dict[\"RR\"] / sum,\n",
    "        \"RB\": actions_dict[\"RB\"] / sum,\n",
    "        \"RW\": actions_dict[\"RW\"] / sum,\n",
    "    }\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to define the function that calculates the states for us. The matrix looks like this:\n",
    "[[x, x, x, x, x, x, SB],\n",
    " [x, x, x, x, x, x, SR],\n",
    " [x, x, x, x, x, x, SW],\n",
    " [x, x, x, x, x, x, RB],\n",
    " [x, x, x, x, x, x, RR],\n",
    " [x, x, x, x, x, x, RW],\n",
    " [b, x, x, x, x, x, SB],\n",
    " ...\n",
    " ...\n",
    " [b, b, b, b, b, b, RW]]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_action_to_states(state_without_action, action):\n",
    "    copied = list(copy.copy(state_without_action))\n",
    "    copied.append(action)\n",
    "    return np.array(copied)\n",
    "\n",
    "\n",
    "def get_states():\n",
    "    possibilities = [\"x\", \"r\", \"w\", \"b\"]\n",
    "    states_without_actions = [p for p in itertools.product(possibilities, repeat=NUMBER_OF_SLOTS)]\n",
    "\n",
    "    final_states = []\n",
    "    for state_without_action in states_without_actions:\n",
    "        final_states.append(append_action_to_states(state_without_action, \"SB\"))\n",
    "        final_states.append(append_action_to_states(state_without_action, \"SR\"))\n",
    "        final_states.append(append_action_to_states(state_without_action, \"SW\"))\n",
    "        final_states.append(append_action_to_states(state_without_action, \"RB\"))\n",
    "        final_states.append(append_action_to_states(state_without_action, \"RR\"))\n",
    "        final_states.append(append_action_to_states(state_without_action, \"RW\"))\n",
    "\n",
    "    return np.array(final_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create the transition probability matrix. It has to look at the states and see which transitions are possible and has to populate the matrix with the values of the possibilities of certain actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def did_warehouse_change_except_in_slot(warehouse_slot, state_1, state_2):\n",
    "    for i in range(len(state_1)):\n",
    "        if i == warehouse_slot:\n",
    "            continue\n",
    "        if state_1[i] != state_2[i]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_color(state):\n",
    "    return state[NUMBER_OF_SLOTS][1].lower()\n",
    "\n",
    "\n",
    "def is_store_action_possible(warehouse_slot, state_1, state_2):\n",
    "    color_to_store = get_color(state_1)\n",
    "\n",
    "    if did_warehouse_change_except_in_slot(warehouse_slot, state_1, state_2):\n",
    "        return False\n",
    "\n",
    "    if state_1[warehouse_slot] != \"x\" and state_1[warehouse_slot] != state_2[warehouse_slot]:\n",
    "        # slot was overwritten, that is not possible\n",
    "        return False\n",
    "\n",
    "    if state_1[warehouse_slot] == \"x\" and state_2[warehouse_slot] != color_to_store:\n",
    "        # empty slot was overwritten, but with something else we expected, not possible\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_restore_action_possible(warehouse_slot, state_1, state_2):\n",
    "    color_to_restore = get_color(state_1)\n",
    "\n",
    "    if did_warehouse_change_except_in_slot(warehouse_slot, state_1, state_2):\n",
    "        return False\n",
    "\n",
    "    if state_1[warehouse_slot] != color_to_restore and state_1[warehouse_slot] != state_2[warehouse_slot]:\n",
    "        # something changed which had nothing to do with the restore color command, not possible\n",
    "        return False\n",
    "\n",
    "    if state_1[warehouse_slot] == color_to_restore and state_2[warehouse_slot] != \"x\":\n",
    "        # a color was replaced instead of just getting it, not possible\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_transition_possible(warehouse_slot, state_1, state_2):\n",
    "    action = state_1[NUMBER_OF_SLOTS][0]\n",
    "    if action == \"S\":\n",
    "        return is_store_action_possible(warehouse_slot, state_1, state_2)\n",
    "    else:\n",
    "        return is_restore_action_possible(warehouse_slot, state_1, state_2)\n",
    "\n",
    "\n",
    "def distribute_transition_probability_matrix(tpm):\n",
    "    for i_action, action in enumerate(tpm):\n",
    "        for index, row_vector in enumerate(action):\n",
    "            sum_of_probability = np.sum(row_vector)\n",
    "            if sum_of_probability == 0:\n",
    "                # no transition was possible for this state (e. g. restore from empty warehouse)\n",
    "                # it should stay in the state then since the sum of each row has to equal one\n",
    "                tpm[action, index, index] = 1\n",
    "                continue\n",
    "        # give every possible transition the same possibility by dividing every element by the sum\n",
    "        # of the row. E. g. [0, 0, 1, 1, 0, 0] will be converted to [0, 0, 0.5, 0.5, 0, 0].\n",
    "        # Also, for example [0.25, 0.15, 0.3] will be normalized to [0.35, 0.21, 0.44]\n",
    "        # This will also result in the sum of the row equal to one.\n",
    "        tpm[i_action] = action / action.sum(axis=1)[:, None]\n",
    "    return tpm\n",
    "\n",
    "\n",
    "def get_probability(probability_distribution, state):\n",
    "    action = state[NUMBER_OF_SLOTS]\n",
    "    return probability_distribution[action]\n",
    "\n",
    "\n",
    "def get_transition_probability_matrix(probability_distribution, states):\n",
    "    tpm = np.zeros((NUMBER_OF_SLOTS, len(states), len(states)))\n",
    "    for warehouse_slot in range(NUMBER_OF_SLOTS):\n",
    "        for i, state_1 in enumerate(states, start=0):\n",
    "            for j, state_2 in enumerate(states, start=0):\n",
    "                if is_transition_possible(warehouse_slot, state_1, state_2):\n",
    "                    tpm[warehouse_slot, i, j] = get_probability(probability_distribution, state_2)\n",
    "    return distribute_transition_probability_matrix(tpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last preparation step is to create the reward matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reward_for_store_action(state):\n",
    "    reward = np.zeros(NUMBER_OF_SLOTS)\n",
    "\n",
    "    for i in range(NUMBER_OF_SLOTS):\n",
    "        if state[i] != \"x\":\n",
    "            reward[i] = -1\n",
    "        else:\n",
    "            reward[i] = REWARD_MAPPING.get(i)\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_reward_for_restore_action(state):\n",
    "    color_to_restore = get_color(state)\n",
    "    reward = np.zeros(NUMBER_OF_SLOTS)\n",
    "\n",
    "    for i in range(NUMBER_OF_SLOTS):\n",
    "        if state[i] != color_to_restore:\n",
    "            reward[i] = -1\n",
    "        else:\n",
    "            reward[i] = REWARD_MAPPING.get(i)\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_reward_matrix(states):\n",
    "    rm = np.zeros((len(states), NUMBER_OF_SLOTS))\n",
    "    for i, state_1 in enumerate(states, start=0):\n",
    "        action = state_1[NUMBER_OF_SLOTS][0]\n",
    "        if action == \"S\":\n",
    "            reward = get_reward_for_store_action(state_1)\n",
    "        else:\n",
    "            reward = get_reward_for_restore_action(state_1)\n",
    "        rm[i] = reward\n",
    "    return rm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done with the preparation! Let's call the functions, get the components and give them to the MDP Toolbox. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-391003f83374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprobability_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_percentage_of_actions_by_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtpm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_transition_probability_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreward_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reward_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7f0fc384b40b>\u001b[0m in \u001b[0;36mget_transition_probability_matrix\u001b[0;34m(probability_distribution, states)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mis_transition_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarehouse_slot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mtpm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwarehouse_slot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistribute_transition_probability_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7f0fc384b40b>\u001b[0m in \u001b[0;36mis_transition_possible\u001b[0;34m(warehouse_slot, state_1, state_2)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mis_store_action_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarehouse_slot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mis_restore_action_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarehouse_slot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7f0fc384b40b>\u001b[0m in \u001b[0;36mis_restore_action_possible\u001b[0;34m(warehouse_slot, state_1, state_2)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_restore_action_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarehouse_slot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mcolor_to_restore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdid_warehouse_change_except_in_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarehouse_slot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7f0fc384b40b>\u001b[0m in \u001b[0;36mget_color\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNUMBER_OF_SLOTS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "probability_distribution = get_percentage_of_actions_by_training_data(TRAINING_FILE)\n",
    "states = get_states()\n",
    "tpm = get_transition_probability_matrix(probability_distribution, states)\n",
    "reward_matrix = get_reward_matrix(states)\n",
    "\n",
    "mdpResultPolicy = mdptoolbox.mdp.PolicyIteration(tpm, reward_matrix, 0.95, max_iter=len(states) * 5)\n",
    "mdpResultValue = mdptoolbox.mdp.ValueIteration(tpm, reward_matrix, 0.95, max_iter=len(states) * 5)\n",
    "\n",
    "mdpResultPolicy.run()\n",
    "mdpResultValue.run()\n",
    "\n",
    "print('PolicyIteration:')\n",
    "print(mdpResultPolicy.policy)\n",
    "print(mdpResultPolicy.V)\n",
    "print(mdpResultPolicy.iter)\n",
    "\n",
    "print('ValueIteration:')\n",
    "print(mdpResultValue.policy)\n",
    "print(mdpResultValue.V)\n",
    "print(mdpResultValue.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now our policies but what do they mean and do they really improve our warehouse? Let's start by implementing a greedy algorithm, that puts or receives an element always on / from the first possible field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to read the training file and read the storing sequence. It will be an array of the following format: [\"SB\", \"SR\", \"RB\", \"RR\", \"SW\", ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_storing_sequence(file):\n",
    "    storing_sequence = []\n",
    "    with open(file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "        for row in csv_reader:\n",
    "            action = (row[0][0] + row[1][0]).upper()\n",
    "            storing_sequence.append(action)\n",
    "    return storing_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function that initializes our warehouse. The warehouse is initialized empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_warehouse():\n",
    "    return ['x' for i in range(NUMBER_OF_SLOTS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is the greedy algorithm. It searches for the first possible action and gets/puts the item. We gave in COST_MAPPING every index a cost. We use that to measure our distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_first_slot_with_color(warehouse, color):\n",
    "    for i in range(len(warehouse)):\n",
    "        if warehouse[i] == color:\n",
    "            return i\n",
    "        \n",
    "        \n",
    "def find_first_empty_slot(warehouse):\n",
    "    for i in range(len(warehouse)):\n",
    "        if warehouse[i] == \"x\":\n",
    "            return i\n",
    "        \n",
    "\n",
    "def get_distance_using_greedy_algorithm(warehouse, storing_sequence):\n",
    "    distance = 0\n",
    "    for action in storing_sequence:\n",
    "        is_storing_action = action[0] == \"S\"\n",
    "        target_color = action[1].lower()\n",
    "        if is_storing_action:\n",
    "            slot = find_first_empty_slot(warehouse)\n",
    "            warehouse[slot] = target_color\n",
    "        else:\n",
    "            slot = find_first_slot_with_color(warehouse, target_color)\n",
    "            warehouse[slot] = \"x\"\n",
    "        distance += COST_MAPPING.get(slot)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need functions that applies the policy to the state. Therefore, we always have to keep track of the state. If the warehouse is in state [r, x, r, x, SR], we have to find the index for the state in the state-array. Then, we put that index into our policy which says where we should store the red item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_equals_warehouse(state, warehouse, action):\n",
    "    for i in range(len(warehouse)):\n",
    "        if state[i] != warehouse[i]:\n",
    "            return False\n",
    "    return state[NUMBER_OF_SLOTS] == action\n",
    "\n",
    "\n",
    "def get_warehouse_state_index(states, warehouse, action):\n",
    "    for i in range(len(states)):\n",
    "        if state_equals_warehouse(states[i], warehouse, action):\n",
    "            return i\n",
    "\n",
    "\n",
    "def get_distance_using_policy(warehouse, storing_sequence, policy):\n",
    "    distance = 0\n",
    "    for action in storing_sequence:\n",
    "        is_storing_action = action[0] == \"S\"\n",
    "        target_color = action[1].lower()\n",
    "        state_index = get_warehouse_state_index(states, warehouse, action)\n",
    "        target_slot_according_to_policy = policy[state_index]\n",
    "        if is_storing_action:\n",
    "            warehouse[target_slot_according_to_policy] = target_color\n",
    "        else:\n",
    "            warehouse[target_slot_according_to_policy] = \"x\"\n",
    "        distance += COST_MAPPING.get(target_slot_according_to_policy)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is prepared. Start measuring the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storing_sequence = get_storing_sequence(TRAINING_FILE)\n",
    "\n",
    "warehouse = init_warehouse()\n",
    "distance_greedy = get_distance_using_greedy_algorithm(warehouse, storing_sequence)\n",
    "\n",
    "policyValueIteration = list(mdpResultValue.policy)\n",
    "policyPolicyIteration = list(mdpResultPolicy.policy)\n",
    "warehouse = init_warehouse()\n",
    "distance_ValueIteration = get_distance_using_policy(warehouse, storing_sequence, policyValueIteration)\n",
    "warehouse = init_warehouse()\n",
    "distance_PolicyIteration = get_distance_using_policy(warehouse, storing_sequence, policyPolicyIteration)\n",
    "\n",
    "print(\"Greedy: \", distance_greedy)\n",
    "print(\"Value: \", distance_ValueIteration)\n",
    "print(\"Policy: \", distance_PolicyIteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
